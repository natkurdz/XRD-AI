{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a19a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ccd90",
   "metadata": {},
   "source": [
    "We need to get from every sample:\n",
    " - UVW and XY (.new)\n",
    " - kalpha 1 and kalpha 2  (.xrdml)\n",
    " - 2theta (start and end position) (.xrdml)\n",
    " - counts (.xrdml)\n",
    " - background (start, step, end) (.bac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1406d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_names(path):\n",
    "    '''Function to see structure of data in files and catalogs'''\n",
    "    with open(\"names.txt\", \"w\", encoding=\"utf-8\") as names_f:\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            names_f.write(f\"[PATH]{root}\\n\")\n",
    "            for d in dirs: \n",
    "                names_f.write(f\"[DIR]{d}\\n\")\n",
    "            for f in files: \n",
    "                names_f.write(f\"[FILE]{f}\\n\")\n",
    "            names_f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c3de2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_names(extention,ext_file):\n",
    "    '''Function separating files with specific extention'''\n",
    "    new = open(ext_file,'w')\n",
    "    with open(\"names.txt\",'r') as names_f:\n",
    "        for line in names_f:\n",
    "            if '[PATH]' in line:\n",
    "                new.write(f\"{line}\")\n",
    "            elif '[FILE]' and extention in line:\n",
    "                    new.write(f\"{line}\")\n",
    "        new.write(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2106ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_document(ext,ext_file):\n",
    "    document_names(ext,ext_file)\n",
    "    with open(ext_file,'r') as new_file:\n",
    "        file = new_file.read().replace(\"\\\\\",\"/\")\n",
    "    with open(ext_file, \"w\", encoding=\"utf-8\") as new_file:\n",
    "        new_file.write(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68a35ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_new_doc(ext_file):\n",
    "    '''function saving paths and file names in order in variables'''\n",
    "    path,files_names = [],[]\n",
    "    with open(ext_file,'r') as new_doc:\n",
    "        lines = new_doc.readlines()\n",
    "        for i,line in enumerate(lines):\n",
    "            if '[PATH]' in line:\n",
    "                clean_path = line.replace(\"[PATH]\",\"\").strip()\n",
    "                n = 1\n",
    "                if i+n< len(lines) and '[FILE]' in lines[i+n]:\n",
    "                    while '[FILE]' in lines[i+n]:\n",
    "                        clean_file = lines[i+n].replace(\"[FILE]\",\"/\").strip()\n",
    "                        full_path = clean_path+clean_file\n",
    "                        path.append(full_path)\n",
    "                        file_name = lines[i+n].replace(\"[FILE]\",\"\").strip()\n",
    "                        files_names.append(file_name)\n",
    "                        n=n+1\n",
    "    return path,files_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af3e0350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_inside(path):\n",
    "    file = open(str(path),'r')\n",
    "    read_content = file.read()\n",
    "    print(read_content)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72f44181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_xrdml_file(path):\n",
    "    '''saving data from files to variables:\n",
    "        kalpha_theta = ['kAlpha1','kAlpha2','2Theta - start position','2Theta - end position']\n",
    "        counts = ['all the counts']'''\n",
    "    base, _ = os.path.splitext(path)\n",
    "    new_path = base + \".xrdml\"\n",
    "    kalpha1,kalpha2,theta_start,theta_stop,counts = [],[],[],[],[]\n",
    "\n",
    "    if not os.path.exists(new_path):\n",
    "        return False, False \n",
    "    with open(new_path,'r') as new_doc:\n",
    "        numered_lines = new_doc.readlines()\n",
    "        for i,line in enumerate(numered_lines):\n",
    "            if '<kAlpha1 unit=\"Angstrom\">' in line:\n",
    "                kalpha1.append(float(line.split(\">\")[1].split(\"<\")[0]))\n",
    "            if '<kAlpha2 unit=\"Angstrom\">' in line:\n",
    "                kalpha2.append(float(line.split(\">\")[1].split(\"<\")[0]))\n",
    "            if '<positions axis=\"2Theta\" unit=\"deg\">' in line:\n",
    "                theta_start.append(float(numered_lines[i+1].split(\">\")[1].split(\"<\")[0]))\n",
    "                theta_stop.append(float(numered_lines[i+2].split(\">\")[1].split(\"<\")[0]))\n",
    "            if 'counts'in line:\n",
    "                count = line.split(\">\")[1].split(\"<\")[0]\n",
    "                counts = count.split()\n",
    "    for i in range(len(counts)):\n",
    "        counts[i]=int(counts[i])      \n",
    "    return kalpha1,kalpha2,theta_start,theta_stop,counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1460ca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bac_file(path):\n",
    "    '''saving data from background to variables:\n",
    "        start_step_end = ['start','step','end']\n",
    "        counts_bac = ['all the counts from background']'''\n",
    "    base, _ = os.path.splitext(path)\n",
    "    new_path = base + \".bac\"\n",
    "    start_step_end,counts_bac = [],[]\n",
    "\n",
    "    if not os.path.exists(new_path):\n",
    "        return False, False \n",
    "    with open(new_path,'r') as new_doc:\n",
    "        numered_lines = new_doc.readlines()\n",
    "        for i,line in enumerate(numered_lines):\n",
    "            if i == 0:\n",
    "                a = line.split()\n",
    "                for n in range(3):\n",
    "                    start_step_end.append(float(a[n]))\n",
    "            if i != 0:\n",
    "                counts_b = line.split()\n",
    "                for n in range(len(counts_b)):\n",
    "                    counts_bac.append(int(counts_b[n]))\n",
    "    return start_step_end,counts_bac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "697e68a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_new_file(path):\n",
    "    '''function saving uvw and xy\n",
    "        uvw_xy = ['U','V','W','X','Y','GauSiz','LorSiz','Size-Model']\n",
    "        and '''\n",
    "    base, _ = os.path.splitext(path)\n",
    "    new_path = base + \".new\"\n",
    "    index,uvw_xy = [],[]\n",
    "    \n",
    "    if not os.path.exists(new_path):\n",
    "        return False\n",
    "    with open(new_path,'r') as new_doc:\n",
    "        numered_lines = new_doc.readlines()\n",
    "        for i,line in enumerate(numered_lines):\n",
    "            if line.strip() == '!':\n",
    "                continue\n",
    "            if '!' in line:\n",
    "                if i>2:\n",
    "                    index.append(i)  \n",
    "        if numered_lines[-2] != '!  2Th1/TOF1    2Th2/TOF2  Pattern to plot\\n':\n",
    "                    index.append(0)  \n",
    "        fourth_from_end = index[-4]\n",
    "        uvw_xy = numered_lines[fourth_from_end+1].split()\n",
    "    for i in range(len(uvw_xy)):\n",
    "        uvw_xy[i]=float(uvw_xy[i])\n",
    "    return uvw_xy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7750e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_type(variable,type):\n",
    "    for i in range(len(variable)):\n",
    "        variable[i] = type(variable[i])\n",
    "    return variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "310df469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sum_file(path):\n",
    "    '''function saving X and Y parameters after program winplotr fitted the peaks from data \n",
    "    and multiply the uncertainties and scor factor to get real uncertainties \n",
    "    - x and y is needed to calculate size and strain from lorencian component\n",
    "    [x and, u(x), y and, u(y)] '''\n",
    "    base, _ = os.path.splitext(path)\n",
    "    new_path = base + \".sum\"\n",
    "    x_y_fitted = []\n",
    "    if not os.path.exists(new_path):\n",
    "        return False\n",
    "    with open(new_path,'r') as new_doc:\n",
    "        numered_lines = new_doc.readlines()\n",
    "        for i,line in enumerate(numered_lines):\n",
    "            if \"=> X and y parameters\" in line:\n",
    "                x_and_ux = line.split()\n",
    "                y_and_uy = numered_lines[i+1].split()\n",
    "            if \"=> Scor:\" in line:\n",
    "                factor =  line.split()\n",
    "        factor = factor[-1]\n",
    "        x_y_fitted.append(x_and_ux[-2])\n",
    "        x_y_fitted.append(x_and_ux[-1])\n",
    "        x_y_fitted.append(y_and_uy[0])\n",
    "        x_y_fitted.append(y_and_uy[1])\n",
    "    change_type(x_y_fitted,float)\n",
    "    x_y_fitted[1] = x_y_fitted[1]*float(factor)\n",
    "    x_y_fitted[3] = x_y_fitted[3]*float(factor)\n",
    "    return x_y_fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e8e2ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hkl_file(path):\n",
    "    '''function saving  all the data from file to seperate variables and after that - as dictionary'''\n",
    "    base, _ = os.path.splitext(path)\n",
    "    new_path = base + \".hkl\"\n",
    "    h,k,l,mult,sinT_lamb,tt,fwhm,f2,sf2,hedlines,data,index,json_structure = [],[],[],[],[],[],[],[],[],[],[],[],[]\n",
    "\n",
    "    if not os.path.exists(new_path):\n",
    "        return False\n",
    "    with open(new_path,'r') as new_doc:\n",
    "        numered_lines = new_doc.readlines()\n",
    "        for i,line in enumerate(numered_lines):\n",
    "            if 'Pattern#' in line: \n",
    "                index.append(i)\n",
    "        index.append(len(numered_lines))\n",
    "\n",
    "        if len(index)>1:\n",
    "            for n in range(len(index)-1):\n",
    "                for i,line in enumerate(numered_lines[index[n]:index[n+1]]):\n",
    "                    if i>1:\n",
    "                        if i==2:\n",
    "                            hedlines.append(line.split())\n",
    "                        else:\n",
    "                            data.append(line.split())\n",
    "                for m in range(index[n+1] - index[n]-3):                    \n",
    "                    h.append(data[m][0])\n",
    "                    k.append(data[m][1])\n",
    "                    l.append(data[m][2])\n",
    "                    mult.append(data[m][3])\n",
    "                    sinT_lamb.append(data[m][4])\n",
    "                    tt.append(data[m][5])\n",
    "                    fwhm.append(data[m][6])\n",
    "                    f2.append(data[m][7])\n",
    "                    sf2.append(data[m][8])\n",
    "                variables = [h,k,l,mult,sinT_lamb,tt,fwhm,f2,sf2]\n",
    "                for i in range(len(variables)):\n",
    "                    if i<4:\n",
    "                        change_type(variables[i],int)\n",
    "                    else:\n",
    "                        change_type(variables[i],float)\n",
    "                json_structure.append({\n",
    "                        \"h\": h,\n",
    "                        \"k\": k,\n",
    "                        \"l\": l,\n",
    "                        \"mult\": mult,\n",
    "                        \"sinT_lamb\": sinT_lamb,\n",
    "                        \"tt\": tt,\n",
    "                        \"fwhm\": fwhm,\n",
    "                        \"f2\": f2,\n",
    "                        \"sf2\": sf2\n",
    "                    })\n",
    "                h,k,l,mult,sinT_lamb,tt,fwhm,f2,sf2,data = [],[],[],[],[],[],[],[],[],[]\n",
    "        else:\n",
    "            for i,line in enumerate(numered_lines):\n",
    "                    if i>1:\n",
    "                        if i==2:\n",
    "                            hedlines.append(line.split())\n",
    "                        else:\n",
    "                            data.append(line.split())\n",
    "            for m in range(len(numered_lines)-3):\n",
    "                h.append(data[m][0])\n",
    "                k.append(data[m][1])\n",
    "                l.append(data[m][2])\n",
    "                mult.append(data[m][3])\n",
    "                sinT_lamb.append(data[m][4])\n",
    "                tt.append(data[m][5])\n",
    "                fwhm.append(data[m][6])\n",
    "                f2.append(data[m][7])\n",
    "                sf2.append(data[m][8])\n",
    "            variables = [h,k,l,mult,sinT_lamb,tt,fwhm,f2,sf2]\n",
    "            for i in range(len(variables)):\n",
    "                if i<4:\n",
    "                    change_type(variables[i],int)\n",
    "                else:\n",
    "                    change_type(variables[i],float)\n",
    "            json_structure = {\n",
    "                    \"h\": h,\n",
    "                    \"k\": k,\n",
    "                    \"l\": l,\n",
    "                    \"mult\": mult,\n",
    "                    \"sinT_lamb\": sinT_lamb,\n",
    "                    \"tt\": tt,\n",
    "                    \"fwhm\": fwhm,\n",
    "                    \"f2\": f2,\n",
    "                    \"sf2\": sf2\n",
    "                }\n",
    "    return json_structure\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89f81f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_storage_save(path,name):  \n",
    "    ''' function using other function to create a file with all the most important data'''\n",
    "    only_name, _ = os.path.splitext(name) \n",
    "    kalpha1,kalpha2,theta_start,theta_stop,counts = find_xrdml_file(path)\n",
    "    uvw_xy = find_new_file(path) \n",
    "    start_step_end,counts_bac = find_bac_file(path)\n",
    "    hkl = find_hkl_file(path)\n",
    "    x_y_fitted = find_sum_file(path)\n",
    "    counts_str = json.dumps(counts, ensure_ascii=False)\n",
    "    counts_bac_str = json.dumps(counts_bac, ensure_ascii=False)\n",
    "    #żeby później wczytać JSON i dostać z powrotem listę, trzeba  użyć json.loads(counts_str).\n",
    "    all_data = {\"name\": only_name,\"data\": {\n",
    "            \"path\" : path,\n",
    "            \"kalpha1\" : kalpha1,\n",
    "            \"kalpha2\" : kalpha2,\n",
    "            \"theta_start\" : theta_start, \n",
    "            \"theta_stop\" : theta_stop, \n",
    "            \"uvw_xy\" : uvw_xy, \n",
    "            \"start_step_end\" : start_step_end,\n",
    "            \"counts\" : counts_str,\n",
    "            \"counts_bac\" : counts_bac_str,  \n",
    "            \"hkl\" : hkl,\n",
    "            \"x_y_fitted\" : x_y_fitted\n",
    "        }\n",
    "    }\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fe0572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created and filled with data.\n"
     ]
    }
   ],
   "source": [
    "path = r\"D:/nai/Nati/studia/inzynierka/github/XRD-AI/Dane\"\n",
    "# path =r\"D:/nai/Nati/studia/LabyNMiT/laby 2 - gondek/gr04/Gr04\"\n",
    "files_names(path)\n",
    "extentions = ['.xrdml','.new','.bac','hkl','sum']\n",
    "extentions_files = ['xrdml_files.txt','new_files.txt','bac_files.txt','hkl_files.txt','sum_files.txt']\n",
    "all_paths,all_names,all_data =[],[],[]\n",
    "for i in range(len(extentions)):\n",
    "    generate_document(extentions[i],extentions_files[i])\n",
    "    paths,names = read_new_doc(extentions_files[i]) \n",
    "    all_paths.append(paths)\n",
    "    all_names.append(names)     \n",
    "length_names = len(all_names[0][:])           \n",
    "length_paths = len(all_paths)   \n",
    "\n",
    "with open('data_storage.json', 'w') as data_storage:\n",
    "    data_storage.write('')\n",
    "for i in range(length_names):\n",
    "    all_data.append(data_storage_save(all_paths[0][i],all_names[0][i]))\n",
    "\n",
    "# all_data.append(data_storage_save(all_paths[0][0],all_names[0][0]))\n",
    "\n",
    "with open('data_storage.json', 'a', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(all_data, ensure_ascii=False, indent=2))\n",
    "    f.write('\\n')\n",
    "print(\"File created and filled with data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5244d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # print(f\"Dane zapisane do {nazwa_pliku}\")\n",
    "    # result\n",
    "    # print(list_of_dicts)\n",
    "    # with open('data_storage.txt', 'a') as data_storage:\n",
    "    #     # data_storage.write('')\n",
    "    #     data_storage.write('[PATH]')\n",
    "    #     data_storage.write(path)\n",
    "    #     data_storage.write('\\n')\n",
    "    #     data_storage.write(only_name)\n",
    "    #     data_storage.write('\\n')\n",
    "    #     # data_storage.write(uvw_xy_hedlines)\n",
    "    #     if uvw_xy== False:\n",
    "    #         data_storage.write('[NO FILE]\\n')\n",
    "    #     else:\n",
    "    #         data_storage.write(str(uvw_xy))\n",
    "    #         data_storage.write('\\n')\n",
    "    #     if kalpha_theta == False:\n",
    "    #         data_storage.write('[NO FILE]\\n')\n",
    "    #         data_storage.write('[NO FILE]\\n')\n",
    "    #     else:\n",
    "    #         data_storage.write(str(kalpha_theta))\n",
    "    #         data_storage.write('\\n')\n",
    "    #         data_storage.write(str(counts))\n",
    "    #         data_storage.write('\\n')\n",
    "    #     if start_step_end == False:\n",
    "    #         data_storage.write('[NO FILE]\\n')\n",
    "    #         data_storage.write('[NO FILE]\\n')\n",
    "    #     else:\n",
    "    #         data_storage.write(str(start_step_end))\n",
    "    #         data_storage.write('\\n')\n",
    "    #         data_storage.write(str(counts_bac))\n",
    "    #         data_storage.write('\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51ca5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def doc_new_inside(path,name):\n",
    "#     '''function saving uvw and xy'''\n",
    "#     inside = []\n",
    "#     index = []\n",
    "#     headline = []\n",
    "#     uvw_xy =[]\n",
    "#     uvw_xy_hedlines =[]\n",
    "#     with open(path,'r') as new_doc:\n",
    "#         inside.append(path)\n",
    "#         numered_lines = new_doc.readlines()\n",
    "#         inside.append(numered_lines[0].strip())\n",
    "#         for i,line in enumerate(numered_lines):\n",
    "#             if line.strip() == '!':\n",
    "#                 continue\n",
    "#             if '!' in line:\n",
    "#                 if i>2:\n",
    "#                     index.append(i)\n",
    "#                 a = line.replace(\"!\",\"\").strip()\n",
    "#                 if i+1< len(numered_lines) and '!' in numered_lines[i+1]:\n",
    "#                     inside.append(a)\n",
    "#                 elif i+1< len(numered_lines):\n",
    "#                     inside.append(a)\n",
    "#                     n=1\n",
    "#                     while i+n<len(numered_lines) and '!' not in numered_lines[i+n]:\n",
    "#                         b = numered_lines[i+n].strip()\n",
    "#                         c=''+b\n",
    "#                         n=n+1\n",
    "#                     inside.append(c)\n",
    "#         print(inside)\n",
    "#         print(index)\n",
    "#         fourth_from_end = index[-4]\n",
    "#         for i,line in enumerate(numered_lines):\n",
    "#             if i in index :\n",
    "#                 h_line = line.lstrip(\"!\").strip()\n",
    "#                 headline.append(h_line)\n",
    "#         uvw_xy_hedlines = numered_lines[fourth_from_end].split()\n",
    "#         uvw_xy = numered_lines[fourth_from_end+1].split()\n",
    "#     print(uvw_xy)\n",
    "#     print(headline)\n",
    "#     with open(\"test.txt\", \"w\", encoding=\"utf-8\") as test:\n",
    "#         test.write(str(inside))  \n",
    "#     only_name, _ = os.path.splitext(name)\n",
    "#     csv_name = only_name + \".csv\"\n",
    "#     with open(csv_name, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "#         writer = csv.writer(csv_file)\n",
    "\n",
    "\n",
    "\n",
    "# doc_new_inside(all_paths[0],all_names[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b1a9b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def all_data_files_of_sample(path,name):\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "#         lines = txt_file.readlines()\n",
    "#     only_name, _ = os.path.splitext(name)\n",
    "#     csv_name = only_name + \".csv\"\n",
    "#     with open('names.txt', \"r\", encoding=\"utf-8\") as base:\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bbc41ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# def save_to_csv(path,name):\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "#         lines = txt_file.readlines()\n",
    "#     only_name, _ = os.path.splitext(name)\n",
    "#     csv_name = only_name + \".csv\"\n",
    "#     with open(csv_name, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "#         writer = csv.writer(csv_file)\n",
    "#         hedlines = ['Title','Current global Chi2 (Bragg contrib.)','Files names']\n",
    "#         lines[0].strip()\n",
    "#         # written = False\n",
    "#         # for line in lines:\n",
    "#         #     line = line.strip()\n",
    "#         #     if not line:\n",
    "#         #         continue\n",
    "\n",
    "#         #     if line.startswith(\"!\") and not written:\n",
    "#         #         headline = line.lstrip(\"!\").strip()\n",
    "#         #         columns = headline.split()\n",
    "#         #         writer.writerow(columns)\n",
    "#         #         written = True\n",
    "#         #         continue\n",
    "\n",
    "#         #     if not line.startswith(\"!\"):\n",
    "#         #         parts = line.split()\n",
    "#         #         if all(c.replace(\".\", \"\", 1).replace(\"-\", \"\", 1).isdigit() for c in parts):\n",
    "#         #             writer.writerow(parts)\n",
    "\n",
    "# save_to_csv(all_paths[0],all_names[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
